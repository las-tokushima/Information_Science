---
title: "Chapter4"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 4-1-3　推測統計学の基礎


```{r}
# p.117
x = seq(20, 80, by=0.2) 
# 代入記号は <- としてもよい x <- seq(20, 80, by=0.2) と等価
plot(x, dnorm(x, 50, 10), type="l", col="blue")
```


### Rを使ったランダム・サンプリング

```{r}
# p.122
SampMean = numeric(length=10000)
for(i in 1:10000){
  # rnorm()は正規分布に従う乱数を返す
  sample = rnorm(16, 6300, 1700)
  # 乱数列の平均値を記録していく
  SampMean[i] = mean(sample)
}
hist(SampMean)
```

## 4-1-4　統計的検定の基礎


```{r}
# p.132
# 2X2の行列を生成
cross = matrix(c(10, 10, 26, 14), 2, 2) 
# 行と列に名前を設定
rownames(cross) = c("P", "N") 
colnames(cross) = c("A", "B") 
chisq.test(cross, correct=FALSE)
```

### 2 つのグループ（サンプル）の平均値の差の検定

```{r}
# p.136
before = c(0.1, 0.3, 0.3, 0.1, 0.2, 0.3, 0.6, 0.4, 0.2, 0.3) 
after = c(1.1, 0.7, 0.6, 0.8, 1.3, 0.5, 0.8, 0.7, 0.9, 0.5) 
t.test(before, after, paired=TRUE)
```


### 「対応」のない t 検定

```{r}
# p.139
classA = c(54, 55, 52, 48, 50, 38, 41, 40, 53, 52) 
classB = c(67, 63, 50, 60, 61, 69, 43, 58, 36, 29) 
var.test(classB,classA)
```

### 分散が等しい場合

```{r}
# p.141
classA = c(54, 55, 52, 48, 50, 38, 41, 40, 53, 52) 
classB = c(67, 63, 50, 60, 61, 69, 43, 58, 36, 29) 
t.test(classB, classA, var.equal=TRUE)

```

### 分散が等しくない場合


```{r}
# p.142
classA = c(54, 55, 52, 48, 50, 38, 41, 40, 53, 52) 
classB = c(67, 63, 50, 60, 61, 69, 43, 58, 36, 29) 
# Rの t.test関数はWelchがデフォルト 
t.test(classB, classA)
```

### 乱数を利用した検定

```{r}
# p.144
test = c( 54, 55, 52, 48, 50, 38, 41, 40, 53, 52, 67, 63, 50, 60, 61, 69, 43, 58, 36, 29)
class = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1) 
test.diff = diff(by(test, class, mean)) 
n = 10000 
# 次のlapplyはRで繰り返しを行う関数 
# by はデータに条件（ここでは1か0か）別の関数（ここでは平均値）を適用する関数 
# sample関数は、指定されたベクトルからランダム抽出を行う 
examples = unlist(lapply(1:n, function(x){diff(by(test,sample(class, length(class), FALSE),mean))}))
# test.diffの値よりも大きい結果となった個数を求め、その割合を計算 
(sum(abs(examples) > abs(test.diff)))/n

```


## 4-1-5　ROC 解析と推論の評価

```{r}
# p.149
install.packages("ROCR") 
library(ROCR)

Point = c(0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100) 
Class = c(0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1) 
pred = prediction(Point, Class) 
perf = performance(pred, "tpr", "fpr") 
plot(perf, col="Red", lwd = 2)
auc = performance(pred, "auc") 
as.numeric(auc@y.values)
```



# 4-2 モデリング

## 4-2-3　単回帰分析


```{r}
# p.160
# データ表示 
cars
```

```{r}
# p.161
# 散布図を描画
plot(cars)
```

```{r}
# 相関係数を計算 
cor(cars$speed, cars$dist)
```

```{r}
# p.162
# 単回帰分析を実行 
result <- lm(dist ~ speed, data=cars) 
# 単回帰曲線の表示 
plot(cars) 
abline(result, col="red")
```

```{r}
# p.163
# 決定係数などを表示 
summary(result)
```


### R でデータをファイルから読み込む場合

```{r}
# p.165
# 現在の作業フォルダを確認 
getwd()
# 作業フォルダの変更 
# Macの場合（Windowsならば C:/Users/username/Documentsなど）
setwd("/Users/username/Documents") 
# 変更後の作業フォルダを確認 
getwd()

```

```{r}
# p.166
# csvファイルからデータの読み込み 
# GitHub レポジトリの場合、dataフォルダにファイルがある
cars_data <- read.csv("../data/cars.csv", header=TRUE, row.names=1) 
# 格納されたデータの表示 
cars_data
```


## 4-2-4　クラスタ分析

```{r}
# p.179
library(cluster) 
ruspini

```

```{r}
# p.180
# 非類似度を計算 
ruspini-distance <- dist(ruspini)

```

```{r}
# p.181
# 階層的クラスタリングを実行 
ruspini_result <- hclust(ruspini_distance, methodxs="single") 
# 階層的クラスタリングの結果を樹形図として描画 
plot(ruspini_result)
```


```{r}
# p.182
# 色分けするため樹形図よりクラスタを4つに分類 
color4 <- cutree(ruspini_result,k=4) 
#クラスタを色分けした樹形図を表示 
plot(ruspini, pch=20, asp=1, col=color4)

```

### 非階層的クラスタ

```{r}
# p.190
# ライブラリ取り込み 
library(cluster) 
# データ表示 
ruspini


```

```{r}
# p.191
#  k　-平均法実行
km <- kmeans(ruspini,4)
# 実行結果のクラスタ番号の確認 
km$cluster 
# クラスタ番号をもとに散布図を色分けして描画 
plot(ruspini, pch=20, asp=1, col=km$cluster)
```



```{r}
# p.192
01
#  k　-平均法を1000回実行
km <- kmeans(ruspini,4, iter.max = 1000) 
# クラスタ番号をもとに散布図を色分けして描画 
plot(ruspini, pch=20, asp=1, col=km$cluster)

```

